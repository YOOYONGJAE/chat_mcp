# chatbot/llama_loader.py
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

MODEL_NAME = "google/gemma-2b-it"
tokenizer = None
model = None

def load_model():
    global tokenizer, model
    if tokenizer is None or model is None:
        print("ğŸ§  LLaMA ëª¨ë¸ ì‚¬ì „ ë¡œë”© ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        tokenizer.pad_token = tokenizer.eos_token
        print("âœ… LLaMA ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

def get_model_and_tokenizer():
    if tokenizer is None or model is None:
        print("[WARN] ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•„ load_model()ì„ ìë™ í˜¸ì¶œí•©ë‹ˆë‹¤.")
        load_model()    
    return tokenizer, model