from django.shortcuts import render
from rest_framework.decorators import api_view
from rest_framework.response import Response
import torch
import time
import re
from transformers import GenerationConfig



# âœ… llama_loader.py ì— ìˆëŠ” ëª¨ë¸ ë¡œë”© í•¨ìˆ˜ ê°€ì ¸ì˜¤ê¸°
from .llama_loader import get_model_and_tokenizer
from tools.query_rag import get_rag_prompt

@api_view(['GET'])
def test(request):
    return Response({'message': 'Hello, world!'})

@api_view(['GET', 'POST'])
def chat_test(request):
    print("_ chat_test ì‹¤í–‰")
    question = request.GET.get('question') or request.data.get('question') or ''
    print(f"_ ì§ˆë¬¸ : {question}")

    # âœ… "ëª‡ ê°œ", "ì´ ëª‡ ê°œ", "í”„ë¡œì íŠ¸ ìˆ˜" ê°™ì€ í‘œí˜„ì´ë©´ ì§ì ‘ count
    if re.search(r"(ëª‡\s*ê°œ|ì´\s*\d+\s*ê°œ|í”„ë¡œì íŠ¸\s*(ìˆ˜|ê°¯ìˆ˜|ê°¯ìˆ˜ê°€))", question):
        print("_ í”„ë¡œì íŠ¸ ê°œìˆ˜ ì§ˆë¬¸ìœ¼ë¡œ ë¶„ê¸°")

        # 1. RAG ë¬¸ì„œ ì¤‘ ê°€ì¥ ê´€ë ¨ ìˆëŠ” 1ê°œ ì„ íƒ
        rag_prompt = get_rag_prompt(question, top_k=1)
        print(f"_ rag_prompt : {rag_prompt}")

        # 2. í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•´ì„œ ì¤„ ìˆ˜ ì„¸ê¸°
        # ì¤„ ìˆ˜ ì„¸ë ¤ë©´ "ì •ë³´:" ~ "ë‹µë³€:" ì‚¬ì´ë§Œ ì¶”ì¶œ
        project_block_match = re.search(r"ì •ë³´:\s*(.*?)\s*ì´ì „ì— ë‚˜ì˜¨ ì •ë³´", rag_prompt, re.DOTALL)
        if project_block_match:
            project_block = project_block_match.group(1).strip()
            project_lines = [line for line in project_block.split("\n") if re.match(r"^\d{4}ë…„", line)]
            project_count = len(project_lines)
            answer = f"ì´ {project_count}ê°œì˜ í”„ë¡œì íŠ¸ê°€ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤."
        else:
            answer = "í”„ë¡œì íŠ¸ ì •ë³´ë¥¼ ì •í™•íˆ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        return Response({
            'question': question,
            'answer': answer,
            'timing': {'preprocess': 0.0, 'generation': 0.0, 'total': 0.0}
        })

    # â—ê·¸ ì™¸ ì¼ë°˜ ì§ˆë¬¸ì€ ê¸°ì¡´ RAG + generate ì²˜ë¦¬
    rag_prompt = get_rag_prompt(question, top_k=1)
    print(f"_ rag_prompt : {rag_prompt}")


    #prompt = f"### ì§ˆë¬¸:\n{question}\n\n### ë‹µë³€:\n"
    prompt = rag_prompt

    # print(f"_ prompt : {prompt}")

    # âœ… ì „ì—­ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
    tokenizer, model = get_model_and_tokenizer()

    print("_ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

    # ğŸ”„ ì „ì²˜ë¦¬
    start_all = time.time()
    start_preprocess = time.time()
    print("_ ì „ì²˜ë¦¬ ì‹œì‘")
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=1024)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]
    end_preprocess = time.time()
    print(f"_ ì „ì²˜ë¦¬ ì™„ë£Œ : {end_preprocess - start_preprocess}ì´ˆ")

    # ğŸ¤– ìƒì„±
    start_gen = time.time()

    print("_ GenerationConfig ì˜µì…˜ ì„¤ì •")

    generation_config = GenerationConfig(
    # temperature=0.1,
    # top_k=1,
    do_sample=False,
    # repetition_penalty=1.2,
    max_new_tokens=256,
    # eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.pad_token_id
    )

    print("_ ìƒì„± ì‹œì‘")

    with torch.no_grad():
        output_ids = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            generation_config=generation_config
        )

    end_gen = time.time()
    print(f"_ ìƒì„± ì™„ë£Œ : {end_gen - start_gen}ì´ˆ")
    # ğŸ“¤ ê²°ê³¼ ë””ì½”ë”©
    # output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    # answer = output_text.strip()

    
    # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
    full_output = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)

    # ì²« ë¬¸ë‹¨ê¹Œì§€ë§Œ ì‚¬ìš©
    answer = full_output.strip().split("\n")[0]
    
    print("_answer")
    print(answer)

    end_all = time.time()

    return Response({
        'question': question,
        'answer': answer,
        'timing': {
            'preprocess': round(end_preprocess - start_preprocess, 2),
            'generation': round(end_gen - start_gen, 2),
            'total': round(end_all - start_all, 2)
        }
    })
