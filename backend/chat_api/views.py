from django.shortcuts import render
from rest_framework.decorators import api_view
from rest_framework.response import Response
import torch
import time
import re
from transformers import GenerationConfig



# âœ… llama_loader.py ì— ìˆëŠ” ëª¨ë¸ ë¡œë”© í•¨ìˆ˜ ê°€ì ¸ì˜¤ê¸°
from .llama_loader import get_model_and_tokenizer
from tools.query_rag import get_rag_prompt

@api_view(['GET'])
def test(request):
    return Response({'message': 'Hello, world!'})

@api_view(['GET', 'POST'])
def chat_test(request):
    print("[INFO] chat_test ì‹¤í–‰")
    question = request.GET.get('question') or request.data.get('question') or ''
    print(f"[INFO] ì§ˆë¬¸ : {question}")

    # â—ê·¸ ì™¸ ì¼ë°˜ ì§ˆë¬¸ì€ ê¸°ì¡´ RAG + generate ì²˜ë¦¬
    rag_prompt = get_rag_prompt(question, top_k=4)
    print(f"[INFO] rag_prompt : {rag_prompt}")


    #prompt = f"### ì§ˆë¬¸:\n{question}\n\n### ë‹µë³€:\n"
    prompt = rag_prompt

    # print(f"[INFO] prompt : {prompt}")

    # âœ… ì „ì—­ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
    tokenizer, model = get_model_and_tokenizer()

    print("[INFO] ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

    # ğŸ”„ ì „ì²˜ë¦¬
    start_all = time.time()
    start_preprocess = time.time()
    print("[INFO] ì „ì²˜ë¦¬ ì‹œì‘")
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=1024)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]
    end_preprocess = time.time()
    print(f"[INFO] ì „ì²˜ë¦¬ ì™„ë£Œ : {end_preprocess - start_preprocess}ì´ˆ")

    # ğŸ¤– ìƒì„±
    start_gen = time.time()

    print("[INFO] GenerationConfig ì˜µì…˜ ì„¤ì •")

    generation_config = GenerationConfig(
        temperature=0.1,               # [ìƒ˜í”Œë§ ì˜¨ë„] ë‚®ì„ìˆ˜ë¡ ê²°ì •ì  â†’ í•­ìƒ ë¹„ìŠ·í•œ ë‹µë³€ ìƒì„±ë¨ (0ì— ê°€ê¹Œìš°ë©´ ê±°ì˜ greedy)
        top_k=50,                       # [ìƒìœ„ k í† í° ì œí•œ] í™•ë¥ ì´ ê°€ì¥ ë†’ì€ 1ê°œì˜ í† í°ë§Œ í›„ë³´ë¡œ ì‚¬ìš© (íƒìƒ‰ ë²”ìœ„ ì¶•ì†Œ)
        do_sample=True,              # [ìƒ˜í”Œë§ ì—¬ë¶€] Falseë©´ í™•ë¥ ì´ ê°€ì¥ ë†’ì€ í† í°ì„ í•­ìƒ ì„ íƒ (deterministic)
        max_new_tokens=150,             # [ìµœëŒ€ ìƒì„± í† í° ìˆ˜] ë‹µë³€ì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì œí•œ
        eos_token_id=tokenizer.eos_token_id
    )

    # ğŸ¤– ìƒì„± (with torch.no_grad() ì¶”ê°€)
    start_gen = time.time()
    with torch.no_grad():
        output_ids = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            generation_config=generation_config
        )

    end_gen = time.time()
    print(f"[INFO] ìƒì„± ì™„ë£Œ : {end_gen - start_gen}ì´ˆ")
    # ğŸ“¤ ê²°ê³¼ ë””ì½”ë”©
    # output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    # answer = output_text.strip()

    
    # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
    full_output = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)

    # ì²« ë¬¸ë‹¨ê¹Œì§€ë§Œ ì‚¬ìš©
    answer = full_output.strip().split("\n")[0]
    
    print(f"[INFO] answer : {answer}")    

    end_all = time.time()

    return Response({
        'question': question,
        'answer': answer,
        'timing': {
            'preprocess': round(end_preprocess - start_preprocess, 2),
            'generation': round(end_gen - start_gen, 2),
            'total': round(end_all - start_all, 2)
        }
    })
