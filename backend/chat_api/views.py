from django.shortcuts import render
from rest_framework.decorators import api_view
from rest_framework.response import Response
import torch
import time
import re
from transformers import GenerationConfig



# âœ… llama_loader.py ì— ìˆëŠ” ëª¨ë¸ ë¡œë”© í•¨ìˆ˜ ê°€ì ¸ì˜¤ê¸°
from .llama_loader import get_model_and_tokenizer
from tools.query_rag import get_rag_prompt

@api_view(['GET'])
def test(request):
    return Response({'message': 'Hello, world!'})

@api_view(['GET', 'POST'])
def chat_test(request):
    print("[INFO] chat_test ì‹¤í–‰")
    question = request.GET.get('question') or request.data.get('question') or ''
    print(f"[INFO] ì§ˆë¬¸ : {question}")

    # âœ… "ëª‡ ê°œ", "ì´ ëª‡ ê°œ", "í”„ë¡œì íŠ¸ ìˆ˜" ê°™ì€ í‘œí˜„ì´ë©´ ì§ì ‘ count
    if re.search(r"(ëª‡\s*ê°œ|ì´\s*\d+\s*ê°œ|í”„ë¡œì íŠ¸\s*(ìˆ˜|ê°¯ìˆ˜|ê°¯ìˆ˜ê°€))", question):
        print("[INFO] í”„ë¡œì íŠ¸ ê°œìˆ˜ ì§ˆë¬¸ìœ¼ë¡œ ë¶„ê¸°")

        # 1. RAG ë¬¸ì„œ ì¤‘ ê°€ì¥ ê´€ë ¨ ìˆëŠ” 1ê°œ ì„ íƒ
        rag_prompt = get_rag_prompt(question, top_k=1)
        print(f"[INFO] rag_prompt : {rag_prompt}")

        # 2. í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•´ì„œ ì¤„ ìˆ˜ ì„¸ê¸°
        # ì¤„ ìˆ˜ ì„¸ë ¤ë©´ "ì •ë³´:" ~ "ë‹µë³€:" ì‚¬ì´ë§Œ ì¶”ì¶œ
        project_block_match = re.search(r"ì •ë³´:\s*(.*?)\s*ì´ì „ì— ë‚˜ì˜¨ ì •ë³´", rag_prompt, re.DOTALL)
        if project_block_match:
            project_block = project_block_match.group(1).strip()
            project_lines = [line for line in project_block.split("\n") if re.match(r"^\d{4}ë…„", line)]
            project_count = len(project_lines)
            answer = f"ì´ {project_count}ê°œì˜ í”„ë¡œì íŠ¸ê°€ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤."
        else:
            answer = "í”„ë¡œì íŠ¸ ì •ë³´ë¥¼ ì •í™•íˆ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        return Response({
            'question': question,
            'answer': answer,
            'timing': {'preprocess': 0.0, 'generation': 0.0, 'total': 0.0}
        })

    # â—ê·¸ ì™¸ ì¼ë°˜ ì§ˆë¬¸ì€ ê¸°ì¡´ RAG + generate ì²˜ë¦¬
    rag_prompt = get_rag_prompt(question, top_k=1)
    print(f"[INFO] rag_prompt : {rag_prompt}")


    #prompt = f"### ì§ˆë¬¸:\n{question}\n\n### ë‹µë³€:\n"
    prompt = rag_prompt

    # print(f"[INFO] prompt : {prompt}")

    # âœ… ì „ì—­ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
    tokenizer, model = get_model_and_tokenizer()

    print("[INFO] ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

    # ğŸ”„ ì „ì²˜ë¦¬
    start_all = time.time()
    start_preprocess = time.time()
    print("[INFO] ì „ì²˜ë¦¬ ì‹œì‘")
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=1024)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]
    end_preprocess = time.time()
    print(f"[INFO] ì „ì²˜ë¦¬ ì™„ë£Œ : {end_preprocess - start_preprocess}ì´ˆ")

    # ğŸ¤– ìƒì„±
    start_gen = time.time()

    print("[INFO] GenerationConfig ì˜µì…˜ ì„¤ì •")

    generation_config = GenerationConfig(
        temperature=0.1,               # [ìƒ˜í”Œë§ ì˜¨ë„] ë‚®ì„ìˆ˜ë¡ ê²°ì •ì  â†’ í•­ìƒ ë¹„ìŠ·í•œ ë‹µë³€ ìƒì„±ë¨ (0ì— ê°€ê¹Œìš°ë©´ ê±°ì˜ greedy)
        top_k=1,                       # [ìƒìœ„ k í† í° ì œí•œ] í™•ë¥ ì´ ê°€ì¥ ë†’ì€ 1ê°œì˜ í† í°ë§Œ í›„ë³´ë¡œ ì‚¬ìš© (íƒìƒ‰ ë²”ìœ„ ì¶•ì†Œ)
        do_sample=False,              # [ìƒ˜í”Œë§ ì—¬ë¶€] Falseë©´ í™•ë¥ ì´ ê°€ì¥ ë†’ì€ í† í°ì„ í•­ìƒ ì„ íƒ (deterministic)
        repetition_penalty=1.2,     # [ì¤‘ë³µ ì–µì œ] ë™ì¼ ë‹¨ì–´ ë°˜ë³µì„ ë°©ì§€ (1ë³´ë‹¤ í¬ë©´ íŒ¨ë„í‹° ì ìš©ë¨ â†’ ì˜ˆ: "ê³„ì† ê°™ì€ ë§ ë°˜ë³µ ë°©ì§€")
        max_new_tokens=256,           # [ìµœëŒ€ ìƒì„± ê¸¸ì´] í•œ ë²ˆì— ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ (ì¶œë ¥ ê¸¸ì´ ì œí•œ)
        eos_token_id=tokenizer.eos_token_id,   # [ì¢…ë£Œ í† í° ID] ì´ í† í°ì´ ìƒì„±ë˜ë©´ ë©ˆì¶¤
        pad_token_id=tokenizer.pad_token_id    # [íŒ¨ë”© í† í° ID] ë°°ì¹˜ ë‚´ ê¸¸ì´ ë§ì¶œ ë•Œ ì‚¬ìš©
    )

    print("[INFO] ìƒì„± ì‹œì‘")

    with torch.no_grad():
        output_ids = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            generation_config=generation_config
        )

    end_gen = time.time()
    print(f"[INFO] ìƒì„± ì™„ë£Œ : {end_gen - start_gen}ì´ˆ")
    # ğŸ“¤ ê²°ê³¼ ë””ì½”ë”©
    # output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    # answer = output_text.strip()

    
    # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
    full_output = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)

    # ì²« ë¬¸ë‹¨ê¹Œì§€ë§Œ ì‚¬ìš©
    answer = full_output.strip().split("\n")[0]
    
    print(f"[INFO] answer : {answer}")    

    end_all = time.time()

    return Response({
        'question': question,
        'answer': answer,
        'timing': {
            'preprocess': round(end_preprocess - start_preprocess, 2),
            'generation': round(end_gen - start_gen, 2),
            'total': round(end_all - start_all, 2)
        }
    })
