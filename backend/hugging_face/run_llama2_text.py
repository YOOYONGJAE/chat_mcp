from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import time

# ì „ì²´ ì‹œì‘ ì‹œê°„
start_all = time.time()

# ëª¨ë¸ ì´ë¦„
model_name = "meta-llama/Llama-2-7b-chat-hf"

print("ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...")
start_load = time.time()

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

end_load = time.time()
print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ê±¸ë¦° ì‹œê°„: {:.2f}ì´ˆ)".format(end_load - start_load))

# í…ŒìŠ¤íŠ¸ìš© ì§ˆë¬¸
question = "ì•ˆë…•í•˜ì„¸ìš”!"

# í”„ë¡¬í”„íŠ¸ êµ¬ì„±
prompt = f"### ì§ˆë¬¸:\n{question}\n\n### ë‹µë³€:\n"

tokenizer.pad_token = tokenizer.eos_token  # ê°€ì¥ ì•ˆì „í•œ ë°©ì‹

# ì „ì²˜ë¦¬ ì‹œì‘
start_preprocess = time.time()
inputs = tokenizer(prompt, return_tensors="pt", padding=True)
inputs = {k: v.to(model.device) for k, v in inputs.items()}
input_ids = inputs["input_ids"]
attention_mask = inputs["attention_mask"]
end_preprocess = time.time()

# ìƒì„± ì‹œì‘
print("ğŸ§  í…ìŠ¤íŠ¸ ìƒì„± ì¤‘...")
start_gen = time.time()
with torch.no_grad():
    output_ids = model.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,
        max_new_tokens=256,
        temperature=0.7,
        top_p=0.9,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,
        min_new_tokens=20
    )
end_gen = time.time()

# ì¶œë ¥ ë””ì½”ë”©
output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
print("\nğŸ’¬ ëª¨ë¸ ì‘ë‹µ:", output_text.replace(prompt, "").strip())

# ì „ì²´ ì¢…ë£Œ ì‹œê°„
end_all = time.time()

# ì‹œê°„ ë¡œê·¸ ì¶œë ¥
print("\nâ±ï¸ ì†Œìš” ì‹œê°„ ìš”ì•½:")
print(f" - ëª¨ë¸ ë¡œë”© ì‹œê°„: {end_load - start_load:.2f}ì´ˆ")
print(f" - ì „ì²˜ë¦¬ ì‹œê°„: {end_preprocess - start_preprocess:.2f}ì´ˆ")
print(f" - í…ìŠ¤íŠ¸ ìƒì„± ì‹œê°„: {end_gen - start_gen:.2f}ì´ˆ")
print(f" - ì „ì²´ ì‹¤í–‰ ì‹œê°„: {end_all - start_all:.2f}ì´ˆ")
