# download_llama2.py

from transformers import AutoTokenizer, AutoModelForCausalLM

import torch

print("CUDA ì‚¬ìš© ê°€ëŠ¥:", torch.cuda.is_available())
print("ì‚¬ìš© ì¤‘ì¸ ì¥ì¹˜:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU")

model_name = "meta-llama/Llama-2-7b-chat-hf"

print("ğŸ”„ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘ ì¤‘...")

tokenizer = AutoTokenizer.from_pretrained(model_name)
# model = AutoModelForCausalLM.from_pretrained(model_name) CPU ë°°ì¹˜

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,     # ë” ì ì€ VRAM ì‚¬ìš© (ì–‘ìí™”ëŠ” ì•„ë‹˜)
    device_map="auto"              # GPUì— ìë™ ë°°ì¹˜
)

print("âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")