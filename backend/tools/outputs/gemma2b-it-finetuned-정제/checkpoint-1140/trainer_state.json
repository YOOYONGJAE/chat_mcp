{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 1140,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.04395604395604396,
      "grad_norm": 7.632628440856934,
      "learning_rate": 1.992982456140351e-05,
      "loss": 4.9706,
      "step": 5
    },
    {
      "epoch": 0.08791208791208792,
      "grad_norm": 10.204829216003418,
      "learning_rate": 1.9842105263157895e-05,
      "loss": 5.0427,
      "step": 10
    },
    {
      "epoch": 0.13186813186813187,
      "grad_norm": 6.543023109436035,
      "learning_rate": 1.975438596491228e-05,
      "loss": 4.3929,
      "step": 15
    },
    {
      "epoch": 0.17582417582417584,
      "grad_norm": 4.795411109924316,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 3.9945,
      "step": 20
    },
    {
      "epoch": 0.21978021978021978,
      "grad_norm": 5.662419319152832,
      "learning_rate": 1.9578947368421055e-05,
      "loss": 3.9232,
      "step": 25
    },
    {
      "epoch": 0.26373626373626374,
      "grad_norm": 6.836350440979004,
      "learning_rate": 1.949122807017544e-05,
      "loss": 3.5501,
      "step": 30
    },
    {
      "epoch": 0.3076923076923077,
      "grad_norm": 5.145383358001709,
      "learning_rate": 1.9403508771929826e-05,
      "loss": 3.2291,
      "step": 35
    },
    {
      "epoch": 0.3516483516483517,
      "grad_norm": 5.933045864105225,
      "learning_rate": 1.931578947368421e-05,
      "loss": 3.1728,
      "step": 40
    },
    {
      "epoch": 0.3956043956043956,
      "grad_norm": 4.310506820678711,
      "learning_rate": 1.9228070175438597e-05,
      "loss": 2.8558,
      "step": 45
    },
    {
      "epoch": 0.43956043956043955,
      "grad_norm": 5.162708282470703,
      "learning_rate": 1.9140350877192982e-05,
      "loss": 2.6829,
      "step": 50
    },
    {
      "epoch": 0.4835164835164835,
      "grad_norm": 4.3355326652526855,
      "learning_rate": 1.9052631578947368e-05,
      "loss": 2.63,
      "step": 55
    },
    {
      "epoch": 0.5274725274725275,
      "grad_norm": 5.579771041870117,
      "learning_rate": 1.8964912280701757e-05,
      "loss": 2.347,
      "step": 60
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 4.0481085777282715,
      "learning_rate": 1.8877192982456142e-05,
      "loss": 2.5146,
      "step": 65
    },
    {
      "epoch": 0.6153846153846154,
      "grad_norm": 3.9540536403656006,
      "learning_rate": 1.8789473684210528e-05,
      "loss": 2.1364,
      "step": 70
    },
    {
      "epoch": 0.6593406593406593,
      "grad_norm": 3.6145615577697754,
      "learning_rate": 1.8701754385964913e-05,
      "loss": 1.9052,
      "step": 75
    },
    {
      "epoch": 0.7032967032967034,
      "grad_norm": 5.136443138122559,
      "learning_rate": 1.86140350877193e-05,
      "loss": 1.8857,
      "step": 80
    },
    {
      "epoch": 0.7472527472527473,
      "grad_norm": 5.177034854888916,
      "learning_rate": 1.8526315789473684e-05,
      "loss": 1.9232,
      "step": 85
    },
    {
      "epoch": 0.7912087912087912,
      "grad_norm": 4.925631046295166,
      "learning_rate": 1.843859649122807e-05,
      "loss": 1.684,
      "step": 90
    },
    {
      "epoch": 0.8351648351648352,
      "grad_norm": 4.752687454223633,
      "learning_rate": 1.835087719298246e-05,
      "loss": 1.4004,
      "step": 95
    },
    {
      "epoch": 0.8791208791208791,
      "grad_norm": 4.3822340965271,
      "learning_rate": 1.8263157894736844e-05,
      "loss": 1.3002,
      "step": 100
    },
    {
      "epoch": 0.9230769230769231,
      "grad_norm": 3.7708144187927246,
      "learning_rate": 1.817543859649123e-05,
      "loss": 1.3009,
      "step": 105
    },
    {
      "epoch": 0.967032967032967,
      "grad_norm": 4.843085765838623,
      "learning_rate": 1.8087719298245615e-05,
      "loss": 1.1255,
      "step": 110
    },
    {
      "epoch": 1.0087912087912088,
      "grad_norm": 4.558256149291992,
      "learning_rate": 1.8e-05,
      "loss": 0.9967,
      "step": 115
    },
    {
      "epoch": 1.0527472527472528,
      "grad_norm": 3.6430983543395996,
      "learning_rate": 1.7912280701754386e-05,
      "loss": 0.958,
      "step": 120
    },
    {
      "epoch": 1.0967032967032968,
      "grad_norm": 4.5930047035217285,
      "learning_rate": 1.7824561403508775e-05,
      "loss": 0.8737,
      "step": 125
    },
    {
      "epoch": 1.1406593406593406,
      "grad_norm": 4.398837089538574,
      "learning_rate": 1.773684210526316e-05,
      "loss": 0.717,
      "step": 130
    },
    {
      "epoch": 1.1846153846153846,
      "grad_norm": 3.7747857570648193,
      "learning_rate": 1.7649122807017546e-05,
      "loss": 0.6048,
      "step": 135
    },
    {
      "epoch": 1.2285714285714286,
      "grad_norm": 3.379318952560425,
      "learning_rate": 1.756140350877193e-05,
      "loss": 0.5595,
      "step": 140
    },
    {
      "epoch": 1.2725274725274724,
      "grad_norm": 8.305323600769043,
      "learning_rate": 1.7473684210526317e-05,
      "loss": 0.5482,
      "step": 145
    },
    {
      "epoch": 1.3164835164835165,
      "grad_norm": 3.1205520629882812,
      "learning_rate": 1.7385964912280702e-05,
      "loss": 0.3963,
      "step": 150
    },
    {
      "epoch": 1.3604395604395605,
      "grad_norm": 3.623042345046997,
      "learning_rate": 1.729824561403509e-05,
      "loss": 0.4974,
      "step": 155
    },
    {
      "epoch": 1.4043956043956043,
      "grad_norm": 3.9419546127319336,
      "learning_rate": 1.7210526315789477e-05,
      "loss": 0.3447,
      "step": 160
    },
    {
      "epoch": 1.4483516483516483,
      "grad_norm": 2.6911985874176025,
      "learning_rate": 1.7122807017543862e-05,
      "loss": 0.3277,
      "step": 165
    },
    {
      "epoch": 1.4923076923076923,
      "grad_norm": 2.6413733959198,
      "learning_rate": 1.7035087719298248e-05,
      "loss": 0.1998,
      "step": 170
    },
    {
      "epoch": 1.5362637362637361,
      "grad_norm": 2.2236506938934326,
      "learning_rate": 1.6947368421052633e-05,
      "loss": 0.1979,
      "step": 175
    },
    {
      "epoch": 1.5802197802197804,
      "grad_norm": 3.3767521381378174,
      "learning_rate": 1.685964912280702e-05,
      "loss": 0.2373,
      "step": 180
    },
    {
      "epoch": 1.6241758241758242,
      "grad_norm": 7.302035331726074,
      "learning_rate": 1.6771929824561408e-05,
      "loss": 0.2139,
      "step": 185
    },
    {
      "epoch": 1.668131868131868,
      "grad_norm": 5.802374362945557,
      "learning_rate": 1.6684210526315793e-05,
      "loss": 0.1904,
      "step": 190
    },
    {
      "epoch": 1.7120879120879122,
      "grad_norm": 2.0205039978027344,
      "learning_rate": 1.659649122807018e-05,
      "loss": 0.1488,
      "step": 195
    },
    {
      "epoch": 1.756043956043956,
      "grad_norm": 1.628744125366211,
      "learning_rate": 1.6508771929824564e-05,
      "loss": 0.0727,
      "step": 200
    },
    {
      "epoch": 1.8,
      "grad_norm": 1.8950525522232056,
      "learning_rate": 1.642105263157895e-05,
      "loss": 0.1984,
      "step": 205
    },
    {
      "epoch": 1.843956043956044,
      "grad_norm": 3.969345808029175,
      "learning_rate": 1.6333333333333335e-05,
      "loss": 0.298,
      "step": 210
    },
    {
      "epoch": 1.8879120879120879,
      "grad_norm": 0.6059702634811401,
      "learning_rate": 1.624561403508772e-05,
      "loss": 0.1464,
      "step": 215
    },
    {
      "epoch": 1.9318681318681319,
      "grad_norm": 1.4391834735870361,
      "learning_rate": 1.6157894736842106e-05,
      "loss": 0.156,
      "step": 220
    },
    {
      "epoch": 1.975824175824176,
      "grad_norm": 4.457927703857422,
      "learning_rate": 1.6070175438596495e-05,
      "loss": 0.2082,
      "step": 225
    },
    {
      "epoch": 2.0175824175824175,
      "grad_norm": 1.4818613529205322,
      "learning_rate": 1.598245614035088e-05,
      "loss": 0.1728,
      "step": 230
    },
    {
      "epoch": 2.0615384615384613,
      "grad_norm": 3.273311138153076,
      "learning_rate": 1.5894736842105266e-05,
      "loss": 0.0655,
      "step": 235
    },
    {
      "epoch": 2.1054945054945056,
      "grad_norm": 5.050691604614258,
      "learning_rate": 1.580701754385965e-05,
      "loss": 0.0529,
      "step": 240
    },
    {
      "epoch": 2.1494505494505494,
      "grad_norm": 1.5217852592468262,
      "learning_rate": 1.5719298245614037e-05,
      "loss": 0.0421,
      "step": 245
    },
    {
      "epoch": 2.1934065934065936,
      "grad_norm": 2.2277445793151855,
      "learning_rate": 1.5631578947368422e-05,
      "loss": 0.0467,
      "step": 250
    },
    {
      "epoch": 2.2373626373626374,
      "grad_norm": 1.0945029258728027,
      "learning_rate": 1.5543859649122808e-05,
      "loss": 0.1539,
      "step": 255
    },
    {
      "epoch": 2.281318681318681,
      "grad_norm": 0.3330499529838562,
      "learning_rate": 1.5456140350877193e-05,
      "loss": 0.0116,
      "step": 260
    },
    {
      "epoch": 2.3252747252747255,
      "grad_norm": 0.33633920550346375,
      "learning_rate": 1.536842105263158e-05,
      "loss": 0.1969,
      "step": 265
    },
    {
      "epoch": 2.3692307692307693,
      "grad_norm": 1.7622544765472412,
      "learning_rate": 1.5280701754385968e-05,
      "loss": 0.1458,
      "step": 270
    },
    {
      "epoch": 2.413186813186813,
      "grad_norm": 8.558029174804688,
      "learning_rate": 1.5192982456140353e-05,
      "loss": 0.1499,
      "step": 275
    },
    {
      "epoch": 2.4571428571428573,
      "grad_norm": 3.779808521270752,
      "learning_rate": 1.5105263157894739e-05,
      "loss": 0.093,
      "step": 280
    },
    {
      "epoch": 2.501098901098901,
      "grad_norm": 0.19316373765468597,
      "learning_rate": 1.5017543859649124e-05,
      "loss": 0.0056,
      "step": 285
    },
    {
      "epoch": 2.545054945054945,
      "grad_norm": 3.931030035018921,
      "learning_rate": 1.492982456140351e-05,
      "loss": 0.3403,
      "step": 290
    },
    {
      "epoch": 2.589010989010989,
      "grad_norm": 0.3251844644546509,
      "learning_rate": 1.4842105263157895e-05,
      "loss": 0.0048,
      "step": 295
    },
    {
      "epoch": 2.632967032967033,
      "grad_norm": 6.563008785247803,
      "learning_rate": 1.475438596491228e-05,
      "loss": 0.1976,
      "step": 300
    },
    {
      "epoch": 2.676923076923077,
      "grad_norm": 2.97345232963562,
      "learning_rate": 1.4666666666666666e-05,
      "loss": 0.2721,
      "step": 305
    },
    {
      "epoch": 2.720879120879121,
      "grad_norm": 1.038971185684204,
      "learning_rate": 1.4578947368421055e-05,
      "loss": 0.0374,
      "step": 310
    },
    {
      "epoch": 2.764835164835165,
      "grad_norm": 7.099576473236084,
      "learning_rate": 1.449122807017544e-05,
      "loss": 0.1223,
      "step": 315
    },
    {
      "epoch": 2.8087912087912086,
      "grad_norm": 0.5814926028251648,
      "learning_rate": 1.4403508771929826e-05,
      "loss": 0.0297,
      "step": 320
    },
    {
      "epoch": 2.852747252747253,
      "grad_norm": 1.4004911184310913,
      "learning_rate": 1.4315789473684212e-05,
      "loss": 0.0483,
      "step": 325
    },
    {
      "epoch": 2.8967032967032966,
      "grad_norm": 2.472564935684204,
      "learning_rate": 1.4228070175438597e-05,
      "loss": 0.1086,
      "step": 330
    },
    {
      "epoch": 2.940659340659341,
      "grad_norm": 0.3546336591243744,
      "learning_rate": 1.4140350877192983e-05,
      "loss": 0.0821,
      "step": 335
    },
    {
      "epoch": 2.9846153846153847,
      "grad_norm": 4.938849925994873,
      "learning_rate": 1.4052631578947368e-05,
      "loss": 0.25,
      "step": 340
    },
    {
      "epoch": 3.0263736263736263,
      "grad_norm": 0.19567838311195374,
      "learning_rate": 1.3964912280701755e-05,
      "loss": 0.0083,
      "step": 345
    },
    {
      "epoch": 3.0703296703296705,
      "grad_norm": 1.8442906141281128,
      "learning_rate": 1.3877192982456143e-05,
      "loss": 0.1167,
      "step": 350
    },
    {
      "epoch": 3.1142857142857143,
      "grad_norm": 4.092510223388672,
      "learning_rate": 1.3789473684210528e-05,
      "loss": 0.1912,
      "step": 355
    },
    {
      "epoch": 3.158241758241758,
      "grad_norm": 3.333314895629883,
      "learning_rate": 1.3701754385964914e-05,
      "loss": 0.0872,
      "step": 360
    },
    {
      "epoch": 3.2021978021978024,
      "grad_norm": 0.1457909494638443,
      "learning_rate": 1.3614035087719299e-05,
      "loss": 0.0038,
      "step": 365
    },
    {
      "epoch": 3.246153846153846,
      "grad_norm": 0.44140809774398804,
      "learning_rate": 1.3526315789473685e-05,
      "loss": 0.1235,
      "step": 370
    },
    {
      "epoch": 3.29010989010989,
      "grad_norm": 2.297497272491455,
      "learning_rate": 1.343859649122807e-05,
      "loss": 0.0227,
      "step": 375
    },
    {
      "epoch": 3.334065934065934,
      "grad_norm": 0.12780290842056274,
      "learning_rate": 1.3350877192982457e-05,
      "loss": 0.0488,
      "step": 380
    },
    {
      "epoch": 3.378021978021978,
      "grad_norm": 4.589734077453613,
      "learning_rate": 1.3263157894736843e-05,
      "loss": 0.1234,
      "step": 385
    },
    {
      "epoch": 3.421978021978022,
      "grad_norm": 0.9696006178855896,
      "learning_rate": 1.317543859649123e-05,
      "loss": 0.0075,
      "step": 390
    },
    {
      "epoch": 3.465934065934066,
      "grad_norm": 3.6690473556518555,
      "learning_rate": 1.3087719298245615e-05,
      "loss": 0.2726,
      "step": 395
    },
    {
      "epoch": 3.50989010989011,
      "grad_norm": 0.13509875535964966,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.0952,
      "step": 400
    },
    {
      "epoch": 3.5538461538461537,
      "grad_norm": 0.08985423296689987,
      "learning_rate": 1.2912280701754386e-05,
      "loss": 0.0531,
      "step": 405
    },
    {
      "epoch": 3.597802197802198,
      "grad_norm": 0.09762627631425858,
      "learning_rate": 1.2824561403508774e-05,
      "loss": 0.0039,
      "step": 410
    },
    {
      "epoch": 3.6417582417582417,
      "grad_norm": 0.06926028430461884,
      "learning_rate": 1.2736842105263159e-05,
      "loss": 0.1292,
      "step": 415
    },
    {
      "epoch": 3.685714285714286,
      "grad_norm": 0.24206703901290894,
      "learning_rate": 1.2649122807017545e-05,
      "loss": 0.0189,
      "step": 420
    },
    {
      "epoch": 3.7296703296703297,
      "grad_norm": 1.8938612937927246,
      "learning_rate": 1.256140350877193e-05,
      "loss": 0.0186,
      "step": 425
    },
    {
      "epoch": 3.7736263736263735,
      "grad_norm": 0.15356554090976715,
      "learning_rate": 1.2473684210526317e-05,
      "loss": 0.028,
      "step": 430
    },
    {
      "epoch": 3.8175824175824173,
      "grad_norm": 0.2776655852794647,
      "learning_rate": 1.2385964912280703e-05,
      "loss": 0.1024,
      "step": 435
    },
    {
      "epoch": 3.8615384615384616,
      "grad_norm": 0.08000427484512329,
      "learning_rate": 1.229824561403509e-05,
      "loss": 0.0255,
      "step": 440
    },
    {
      "epoch": 3.9054945054945054,
      "grad_norm": 0.1689855456352234,
      "learning_rate": 1.2210526315789475e-05,
      "loss": 0.113,
      "step": 445
    },
    {
      "epoch": 3.9494505494505496,
      "grad_norm": 2.6166887283325195,
      "learning_rate": 1.2122807017543861e-05,
      "loss": 0.1066,
      "step": 450
    },
    {
      "epoch": 3.9934065934065934,
      "grad_norm": 0.05833955854177475,
      "learning_rate": 1.2035087719298246e-05,
      "loss": 0.0488,
      "step": 455
    },
    {
      "epoch": 4.035164835164835,
      "grad_norm": 3.0518414974212646,
      "learning_rate": 1.1947368421052632e-05,
      "loss": 0.022,
      "step": 460
    },
    {
      "epoch": 4.079120879120879,
      "grad_norm": 6.258166790008545,
      "learning_rate": 1.1859649122807017e-05,
      "loss": 0.2161,
      "step": 465
    },
    {
      "epoch": 4.123076923076923,
      "grad_norm": 3.0860867500305176,
      "learning_rate": 1.1771929824561406e-05,
      "loss": 0.1122,
      "step": 470
    },
    {
      "epoch": 4.167032967032967,
      "grad_norm": 3.282766342163086,
      "learning_rate": 1.1684210526315792e-05,
      "loss": 0.0754,
      "step": 475
    },
    {
      "epoch": 4.210989010989011,
      "grad_norm": 0.184810608625412,
      "learning_rate": 1.1596491228070177e-05,
      "loss": 0.0539,
      "step": 480
    },
    {
      "epoch": 4.254945054945055,
      "grad_norm": 0.37149348855018616,
      "learning_rate": 1.1508771929824563e-05,
      "loss": 0.094,
      "step": 485
    },
    {
      "epoch": 4.298901098901099,
      "grad_norm": 0.19283495843410492,
      "learning_rate": 1.1421052631578948e-05,
      "loss": 0.0196,
      "step": 490
    },
    {
      "epoch": 4.3428571428571425,
      "grad_norm": 0.08094798028469086,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 0.0147,
      "step": 495
    },
    {
      "epoch": 4.386813186813187,
      "grad_norm": 0.08714567124843597,
      "learning_rate": 1.124561403508772e-05,
      "loss": 0.0613,
      "step": 500
    },
    {
      "epoch": 4.430769230769231,
      "grad_norm": 3.76997971534729,
      "learning_rate": 1.1157894736842105e-05,
      "loss": 0.1162,
      "step": 505
    },
    {
      "epoch": 4.474725274725275,
      "grad_norm": 0.044161807745695114,
      "learning_rate": 1.1070175438596494e-05,
      "loss": 0.0631,
      "step": 510
    },
    {
      "epoch": 4.518681318681319,
      "grad_norm": 4.210544109344482,
      "learning_rate": 1.098245614035088e-05,
      "loss": 0.1547,
      "step": 515
    },
    {
      "epoch": 4.562637362637362,
      "grad_norm": 2.674591064453125,
      "learning_rate": 1.0894736842105265e-05,
      "loss": 0.1213,
      "step": 520
    },
    {
      "epoch": 4.606593406593406,
      "grad_norm": 0.09704169631004333,
      "learning_rate": 1.080701754385965e-05,
      "loss": 0.0017,
      "step": 525
    },
    {
      "epoch": 4.650549450549451,
      "grad_norm": 0.12156402319669724,
      "learning_rate": 1.0719298245614036e-05,
      "loss": 0.0485,
      "step": 530
    },
    {
      "epoch": 4.694505494505495,
      "grad_norm": 0.14506196975708008,
      "learning_rate": 1.0631578947368421e-05,
      "loss": 0.0021,
      "step": 535
    },
    {
      "epoch": 4.7384615384615385,
      "grad_norm": 0.11218210309743881,
      "learning_rate": 1.0543859649122807e-05,
      "loss": 0.048,
      "step": 540
    },
    {
      "epoch": 4.782417582417582,
      "grad_norm": 2.331880569458008,
      "learning_rate": 1.0456140350877194e-05,
      "loss": 0.0488,
      "step": 545
    },
    {
      "epoch": 4.826373626373626,
      "grad_norm": 5.757595062255859,
      "learning_rate": 1.036842105263158e-05,
      "loss": 0.0642,
      "step": 550
    },
    {
      "epoch": 4.870329670329671,
      "grad_norm": 0.4308496117591858,
      "learning_rate": 1.0280701754385967e-05,
      "loss": 0.0409,
      "step": 555
    },
    {
      "epoch": 4.914285714285715,
      "grad_norm": 0.13844582438468933,
      "learning_rate": 1.0192982456140352e-05,
      "loss": 0.0404,
      "step": 560
    },
    {
      "epoch": 4.958241758241758,
      "grad_norm": 5.06481409072876,
      "learning_rate": 1.0105263157894738e-05,
      "loss": 0.0793,
      "step": 565
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.0756172239780426,
      "learning_rate": 1.0017543859649123e-05,
      "loss": 0.0484,
      "step": 570
    },
    {
      "epoch": 5.043956043956044,
      "grad_norm": 0.46344998478889465,
      "learning_rate": 9.929824561403509e-06,
      "loss": 0.0693,
      "step": 575
    },
    {
      "epoch": 5.087912087912088,
      "grad_norm": 3.821944236755371,
      "learning_rate": 9.842105263157896e-06,
      "loss": 0.0782,
      "step": 580
    },
    {
      "epoch": 5.131868131868132,
      "grad_norm": 0.24360089004039764,
      "learning_rate": 9.754385964912281e-06,
      "loss": 0.0242,
      "step": 585
    },
    {
      "epoch": 5.175824175824176,
      "grad_norm": 0.06546422094106674,
      "learning_rate": 9.666666666666667e-06,
      "loss": 0.0771,
      "step": 590
    },
    {
      "epoch": 5.21978021978022,
      "grad_norm": 6.692684650421143,
      "learning_rate": 9.578947368421054e-06,
      "loss": 0.1265,
      "step": 595
    },
    {
      "epoch": 5.263736263736264,
      "grad_norm": 0.27207785844802856,
      "learning_rate": 9.49122807017544e-06,
      "loss": 0.0296,
      "step": 600
    },
    {
      "epoch": 5.3076923076923075,
      "grad_norm": 0.0920027419924736,
      "learning_rate": 9.403508771929825e-06,
      "loss": 0.0449,
      "step": 605
    },
    {
      "epoch": 5.351648351648351,
      "grad_norm": 0.060158759355545044,
      "learning_rate": 9.315789473684212e-06,
      "loss": 0.0008,
      "step": 610
    },
    {
      "epoch": 5.395604395604396,
      "grad_norm": 0.07381701469421387,
      "learning_rate": 9.228070175438598e-06,
      "loss": 0.0149,
      "step": 615
    },
    {
      "epoch": 5.43956043956044,
      "grad_norm": 0.0953250601887703,
      "learning_rate": 9.140350877192983e-06,
      "loss": 0.0691,
      "step": 620
    },
    {
      "epoch": 5.483516483516484,
      "grad_norm": 4.673946380615234,
      "learning_rate": 9.05263157894737e-06,
      "loss": 0.0443,
      "step": 625
    },
    {
      "epoch": 5.527472527472527,
      "grad_norm": 0.19057518243789673,
      "learning_rate": 8.964912280701756e-06,
      "loss": 0.0436,
      "step": 630
    },
    {
      "epoch": 5.571428571428571,
      "grad_norm": 0.13605496287345886,
      "learning_rate": 8.877192982456141e-06,
      "loss": 0.0642,
      "step": 635
    },
    {
      "epoch": 5.615384615384615,
      "grad_norm": 1.0015085935592651,
      "learning_rate": 8.789473684210527e-06,
      "loss": 0.0291,
      "step": 640
    },
    {
      "epoch": 5.65934065934066,
      "grad_norm": 1.5584715604782104,
      "learning_rate": 8.701754385964914e-06,
      "loss": 0.0682,
      "step": 645
    },
    {
      "epoch": 5.7032967032967035,
      "grad_norm": 0.06103307753801346,
      "learning_rate": 8.6140350877193e-06,
      "loss": 0.0686,
      "step": 650
    },
    {
      "epoch": 5.747252747252747,
      "grad_norm": 0.17775215208530426,
      "learning_rate": 8.526315789473685e-06,
      "loss": 0.0615,
      "step": 655
    },
    {
      "epoch": 5.791208791208791,
      "grad_norm": 4.862475872039795,
      "learning_rate": 8.43859649122807e-06,
      "loss": 0.0657,
      "step": 660
    },
    {
      "epoch": 5.835164835164835,
      "grad_norm": 2.337973117828369,
      "learning_rate": 8.350877192982458e-06,
      "loss": 0.066,
      "step": 665
    },
    {
      "epoch": 5.8791208791208796,
      "grad_norm": 0.13035623729228973,
      "learning_rate": 8.263157894736843e-06,
      "loss": 0.0011,
      "step": 670
    },
    {
      "epoch": 5.923076923076923,
      "grad_norm": 3.9594411849975586,
      "learning_rate": 8.175438596491229e-06,
      "loss": 0.0203,
      "step": 675
    },
    {
      "epoch": 5.967032967032967,
      "grad_norm": 0.06817182153463364,
      "learning_rate": 8.087719298245614e-06,
      "loss": 0.124,
      "step": 680
    },
    {
      "epoch": 6.008791208791209,
      "grad_norm": 3.3633499145507812,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.129,
      "step": 685
    },
    {
      "epoch": 6.052747252747253,
      "grad_norm": 0.06713822484016418,
      "learning_rate": 7.912280701754387e-06,
      "loss": 0.0224,
      "step": 690
    },
    {
      "epoch": 6.096703296703296,
      "grad_norm": 0.8854857087135315,
      "learning_rate": 7.824561403508772e-06,
      "loss": 0.0178,
      "step": 695
    },
    {
      "epoch": 6.140659340659341,
      "grad_norm": 4.309637069702148,
      "learning_rate": 7.736842105263158e-06,
      "loss": 0.1212,
      "step": 700
    },
    {
      "epoch": 6.184615384615385,
      "grad_norm": 1.9845443964004517,
      "learning_rate": 7.649122807017545e-06,
      "loss": 0.0551,
      "step": 705
    },
    {
      "epoch": 6.228571428571429,
      "grad_norm": 6.463715553283691,
      "learning_rate": 7.5614035087719305e-06,
      "loss": 0.1159,
      "step": 710
    },
    {
      "epoch": 6.2725274725274724,
      "grad_norm": 3.347053289413452,
      "learning_rate": 7.473684210526316e-06,
      "loss": 0.0853,
      "step": 715
    },
    {
      "epoch": 6.316483516483516,
      "grad_norm": 0.04630468785762787,
      "learning_rate": 7.385964912280702e-06,
      "loss": 0.0324,
      "step": 720
    },
    {
      "epoch": 6.36043956043956,
      "grad_norm": 1.680527925491333,
      "learning_rate": 7.298245614035089e-06,
      "loss": 0.0149,
      "step": 725
    },
    {
      "epoch": 6.404395604395605,
      "grad_norm": 0.043830085545778275,
      "learning_rate": 7.210526315789474e-06,
      "loss": 0.0626,
      "step": 730
    },
    {
      "epoch": 6.4483516483516485,
      "grad_norm": 0.0992291197180748,
      "learning_rate": 7.1228070175438605e-06,
      "loss": 0.0262,
      "step": 735
    },
    {
      "epoch": 6.492307692307692,
      "grad_norm": 0.1993686705827713,
      "learning_rate": 7.035087719298246e-06,
      "loss": 0.0013,
      "step": 740
    },
    {
      "epoch": 6.536263736263736,
      "grad_norm": 0.0519820861518383,
      "learning_rate": 6.947368421052632e-06,
      "loss": 0.0288,
      "step": 745
    },
    {
      "epoch": 6.58021978021978,
      "grad_norm": 6.3860955238342285,
      "learning_rate": 6.859649122807019e-06,
      "loss": 0.0382,
      "step": 750
    },
    {
      "epoch": 6.624175824175824,
      "grad_norm": 4.634512901306152,
      "learning_rate": 6.771929824561404e-06,
      "loss": 0.0625,
      "step": 755
    },
    {
      "epoch": 6.668131868131868,
      "grad_norm": 4.487096309661865,
      "learning_rate": 6.68421052631579e-06,
      "loss": 0.0746,
      "step": 760
    },
    {
      "epoch": 6.712087912087912,
      "grad_norm": 0.04752150923013687,
      "learning_rate": 6.596491228070177e-06,
      "loss": 0.0742,
      "step": 765
    },
    {
      "epoch": 6.756043956043956,
      "grad_norm": 6.038898468017578,
      "learning_rate": 6.508771929824562e-06,
      "loss": 0.0333,
      "step": 770
    },
    {
      "epoch": 6.8,
      "grad_norm": 0.05692225322127342,
      "learning_rate": 6.421052631578948e-06,
      "loss": 0.0181,
      "step": 775
    },
    {
      "epoch": 6.843956043956044,
      "grad_norm": 0.07424544543027878,
      "learning_rate": 6.333333333333333e-06,
      "loss": 0.0123,
      "step": 780
    },
    {
      "epoch": 6.887912087912088,
      "grad_norm": 0.10550267994403839,
      "learning_rate": 6.245614035087721e-06,
      "loss": 0.0765,
      "step": 785
    },
    {
      "epoch": 6.931868131868132,
      "grad_norm": 0.05609726533293724,
      "learning_rate": 6.157894736842106e-06,
      "loss": 0.0164,
      "step": 790
    },
    {
      "epoch": 6.975824175824176,
      "grad_norm": 0.05009016394615173,
      "learning_rate": 6.070175438596492e-06,
      "loss": 0.0298,
      "step": 795
    },
    {
      "epoch": 7.0175824175824175,
      "grad_norm": 1.925828456878662,
      "learning_rate": 5.982456140350877e-06,
      "loss": 0.0512,
      "step": 800
    },
    {
      "epoch": 7.061538461538461,
      "grad_norm": 0.07046447694301605,
      "learning_rate": 5.8947368421052634e-06,
      "loss": 0.034,
      "step": 805
    },
    {
      "epoch": 7.105494505494505,
      "grad_norm": 0.10181056708097458,
      "learning_rate": 5.80701754385965e-06,
      "loss": 0.001,
      "step": 810
    },
    {
      "epoch": 7.14945054945055,
      "grad_norm": 3.7146098613739014,
      "learning_rate": 5.719298245614035e-06,
      "loss": 0.0482,
      "step": 815
    },
    {
      "epoch": 7.193406593406594,
      "grad_norm": 0.04252612590789795,
      "learning_rate": 5.631578947368422e-06,
      "loss": 0.1212,
      "step": 820
    },
    {
      "epoch": 7.237362637362637,
      "grad_norm": 0.05316907539963722,
      "learning_rate": 5.543859649122807e-06,
      "loss": 0.0315,
      "step": 825
    },
    {
      "epoch": 7.281318681318681,
      "grad_norm": 0.06411485373973846,
      "learning_rate": 5.4561403508771935e-06,
      "loss": 0.0351,
      "step": 830
    },
    {
      "epoch": 7.325274725274725,
      "grad_norm": 0.04936708137392998,
      "learning_rate": 5.36842105263158e-06,
      "loss": 0.0104,
      "step": 835
    },
    {
      "epoch": 7.36923076923077,
      "grad_norm": 2.7807717323303223,
      "learning_rate": 5.280701754385965e-06,
      "loss": 0.0574,
      "step": 840
    },
    {
      "epoch": 7.4131868131868135,
      "grad_norm": 0.06762746721506119,
      "learning_rate": 5.192982456140351e-06,
      "loss": 0.0236,
      "step": 845
    },
    {
      "epoch": 7.457142857142857,
      "grad_norm": 0.13081417977809906,
      "learning_rate": 5.105263157894738e-06,
      "loss": 0.0048,
      "step": 850
    },
    {
      "epoch": 7.501098901098901,
      "grad_norm": 3.0551254749298096,
      "learning_rate": 5.0175438596491235e-06,
      "loss": 0.0662,
      "step": 855
    },
    {
      "epoch": 7.545054945054945,
      "grad_norm": 3.246185302734375,
      "learning_rate": 4.929824561403509e-06,
      "loss": 0.0761,
      "step": 860
    },
    {
      "epoch": 7.589010989010989,
      "grad_norm": 2.3118538856506348,
      "learning_rate": 4.842105263157895e-06,
      "loss": 0.0188,
      "step": 865
    },
    {
      "epoch": 7.6329670329670325,
      "grad_norm": 4.938302516937256,
      "learning_rate": 4.754385964912281e-06,
      "loss": 0.1474,
      "step": 870
    },
    {
      "epoch": 7.676923076923077,
      "grad_norm": 0.04305682331323624,
      "learning_rate": 4.666666666666667e-06,
      "loss": 0.015,
      "step": 875
    },
    {
      "epoch": 7.720879120879121,
      "grad_norm": 0.08333966881036758,
      "learning_rate": 4.578947368421053e-06,
      "loss": 0.0136,
      "step": 880
    },
    {
      "epoch": 7.764835164835165,
      "grad_norm": 0.06735459715127945,
      "learning_rate": 4.491228070175439e-06,
      "loss": 0.014,
      "step": 885
    },
    {
      "epoch": 7.808791208791209,
      "grad_norm": 0.1382516622543335,
      "learning_rate": 4.4035087719298245e-06,
      "loss": 0.0374,
      "step": 890
    },
    {
      "epoch": 7.852747252747252,
      "grad_norm": 0.08147553354501724,
      "learning_rate": 4.315789473684211e-06,
      "loss": 0.0085,
      "step": 895
    },
    {
      "epoch": 7.896703296703297,
      "grad_norm": 0.12967701256275177,
      "learning_rate": 4.228070175438596e-06,
      "loss": 0.1195,
      "step": 900
    },
    {
      "epoch": 7.940659340659341,
      "grad_norm": 4.099477767944336,
      "learning_rate": 4.140350877192983e-06,
      "loss": 0.052,
      "step": 905
    },
    {
      "epoch": 7.984615384615385,
      "grad_norm": 0.1388230323791504,
      "learning_rate": 4.052631578947368e-06,
      "loss": 0.0012,
      "step": 910
    },
    {
      "epoch": 8.026373626373626,
      "grad_norm": 2.59128999710083,
      "learning_rate": 3.9649122807017545e-06,
      "loss": 0.0573,
      "step": 915
    },
    {
      "epoch": 8.07032967032967,
      "grad_norm": 0.17344734072685242,
      "learning_rate": 3.877192982456141e-06,
      "loss": 0.0457,
      "step": 920
    },
    {
      "epoch": 8.114285714285714,
      "grad_norm": 1.1119141578674316,
      "learning_rate": 3.789473684210527e-06,
      "loss": 0.0083,
      "step": 925
    },
    {
      "epoch": 8.158241758241758,
      "grad_norm": 0.09590964019298553,
      "learning_rate": 3.7017543859649123e-06,
      "loss": 0.0559,
      "step": 930
    },
    {
      "epoch": 8.202197802197801,
      "grad_norm": 0.06364282220602036,
      "learning_rate": 3.6140350877192986e-06,
      "loss": 0.0372,
      "step": 935
    },
    {
      "epoch": 8.246153846153845,
      "grad_norm": 0.0981220155954361,
      "learning_rate": 3.5263157894736846e-06,
      "loss": 0.0083,
      "step": 940
    },
    {
      "epoch": 8.29010989010989,
      "grad_norm": 0.08548521995544434,
      "learning_rate": 3.4385964912280705e-06,
      "loss": 0.0009,
      "step": 945
    },
    {
      "epoch": 8.334065934065935,
      "grad_norm": 1.0383391380310059,
      "learning_rate": 3.3508771929824564e-06,
      "loss": 0.0139,
      "step": 950
    },
    {
      "epoch": 8.378021978021978,
      "grad_norm": 4.857517719268799,
      "learning_rate": 3.2631578947368423e-06,
      "loss": 0.0493,
      "step": 955
    },
    {
      "epoch": 8.421978021978022,
      "grad_norm": 0.06657886505126953,
      "learning_rate": 3.1754385964912282e-06,
      "loss": 0.0098,
      "step": 960
    },
    {
      "epoch": 8.465934065934066,
      "grad_norm": 0.10871615260839462,
      "learning_rate": 3.0877192982456146e-06,
      "loss": 0.0145,
      "step": 965
    },
    {
      "epoch": 8.50989010989011,
      "grad_norm": 3.700624704360962,
      "learning_rate": 3e-06,
      "loss": 0.0456,
      "step": 970
    },
    {
      "epoch": 8.553846153846154,
      "grad_norm": 0.08874941617250443,
      "learning_rate": 2.9122807017543864e-06,
      "loss": 0.026,
      "step": 975
    },
    {
      "epoch": 8.597802197802197,
      "grad_norm": 0.048522062599658966,
      "learning_rate": 2.824561403508772e-06,
      "loss": 0.0015,
      "step": 980
    },
    {
      "epoch": 8.641758241758241,
      "grad_norm": 0.08270349353551865,
      "learning_rate": 2.7368421052631583e-06,
      "loss": 0.0658,
      "step": 985
    },
    {
      "epoch": 8.685714285714285,
      "grad_norm": 3.7517011165618896,
      "learning_rate": 2.649122807017544e-06,
      "loss": 0.0644,
      "step": 990
    },
    {
      "epoch": 8.729670329670329,
      "grad_norm": 3.60329270362854,
      "learning_rate": 2.56140350877193e-06,
      "loss": 0.0731,
      "step": 995
    },
    {
      "epoch": 8.773626373626374,
      "grad_norm": 0.9420958757400513,
      "learning_rate": 2.473684210526316e-06,
      "loss": 0.042,
      "step": 1000
    },
    {
      "epoch": 8.817582417582418,
      "grad_norm": 0.09740954637527466,
      "learning_rate": 2.385964912280702e-06,
      "loss": 0.0967,
      "step": 1005
    },
    {
      "epoch": 8.861538461538462,
      "grad_norm": 1.4556008577346802,
      "learning_rate": 2.298245614035088e-06,
      "loss": 0.0054,
      "step": 1010
    },
    {
      "epoch": 8.905494505494506,
      "grad_norm": 0.10098493844270706,
      "learning_rate": 2.2105263157894738e-06,
      "loss": 0.0412,
      "step": 1015
    },
    {
      "epoch": 8.94945054945055,
      "grad_norm": 0.030730076134204865,
      "learning_rate": 2.1228070175438597e-06,
      "loss": 0.0009,
      "step": 1020
    },
    {
      "epoch": 8.993406593406593,
      "grad_norm": 5.850266456604004,
      "learning_rate": 2.0350877192982456e-06,
      "loss": 0.1132,
      "step": 1025
    },
    {
      "epoch": 9.035164835164835,
      "grad_norm": 0.05220174789428711,
      "learning_rate": 1.9473684210526315e-06,
      "loss": 0.0171,
      "step": 1030
    },
    {
      "epoch": 9.079120879120879,
      "grad_norm": 0.11645972728729248,
      "learning_rate": 1.8596491228070177e-06,
      "loss": 0.0257,
      "step": 1035
    },
    {
      "epoch": 9.123076923076923,
      "grad_norm": 0.06709825992584229,
      "learning_rate": 1.7719298245614036e-06,
      "loss": 0.0141,
      "step": 1040
    },
    {
      "epoch": 9.167032967032966,
      "grad_norm": 0.06172971427440643,
      "learning_rate": 1.6842105263157895e-06,
      "loss": 0.0769,
      "step": 1045
    },
    {
      "epoch": 9.21098901098901,
      "grad_norm": 3.084059238433838,
      "learning_rate": 1.5964912280701754e-06,
      "loss": 0.0259,
      "step": 1050
    },
    {
      "epoch": 9.254945054945054,
      "grad_norm": 2.0672190189361572,
      "learning_rate": 1.5087719298245616e-06,
      "loss": 0.0286,
      "step": 1055
    },
    {
      "epoch": 9.2989010989011,
      "grad_norm": 0.07393128424882889,
      "learning_rate": 1.4210526315789475e-06,
      "loss": 0.0156,
      "step": 1060
    },
    {
      "epoch": 9.342857142857143,
      "grad_norm": 0.07949526607990265,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 0.014,
      "step": 1065
    },
    {
      "epoch": 9.386813186813187,
      "grad_norm": 0.07453383505344391,
      "learning_rate": 1.2456140350877193e-06,
      "loss": 0.0368,
      "step": 1070
    },
    {
      "epoch": 9.430769230769231,
      "grad_norm": 0.06655128300189972,
      "learning_rate": 1.1578947368421053e-06,
      "loss": 0.0732,
      "step": 1075
    },
    {
      "epoch": 9.474725274725275,
      "grad_norm": 3.7669260501861572,
      "learning_rate": 1.0701754385964914e-06,
      "loss": 0.0189,
      "step": 1080
    },
    {
      "epoch": 9.518681318681319,
      "grad_norm": 6.678552150726318,
      "learning_rate": 9.824561403508773e-07,
      "loss": 0.0927,
      "step": 1085
    },
    {
      "epoch": 9.562637362637362,
      "grad_norm": 0.16419118642807007,
      "learning_rate": 8.947368421052632e-07,
      "loss": 0.001,
      "step": 1090
    },
    {
      "epoch": 9.606593406593406,
      "grad_norm": 0.04403238371014595,
      "learning_rate": 8.070175438596491e-07,
      "loss": 0.0468,
      "step": 1095
    },
    {
      "epoch": 9.65054945054945,
      "grad_norm": 0.03555712476372719,
      "learning_rate": 7.192982456140352e-07,
      "loss": 0.0088,
      "step": 1100
    },
    {
      "epoch": 9.694505494505494,
      "grad_norm": 2.1134281158447266,
      "learning_rate": 6.315789473684211e-07,
      "loss": 0.0203,
      "step": 1105
    },
    {
      "epoch": 9.73846153846154,
      "grad_norm": 0.08352942019701004,
      "learning_rate": 5.438596491228071e-07,
      "loss": 0.0012,
      "step": 1110
    },
    {
      "epoch": 9.782417582417583,
      "grad_norm": 3.1684205532073975,
      "learning_rate": 4.5614035087719304e-07,
      "loss": 0.0632,
      "step": 1115
    },
    {
      "epoch": 9.826373626373627,
      "grad_norm": 3.259092330932617,
      "learning_rate": 3.6842105263157896e-07,
      "loss": 0.1013,
      "step": 1120
    },
    {
      "epoch": 9.87032967032967,
      "grad_norm": 3.750441789627075,
      "learning_rate": 2.8070175438596494e-07,
      "loss": 0.0222,
      "step": 1125
    },
    {
      "epoch": 9.914285714285715,
      "grad_norm": 0.13326102495193481,
      "learning_rate": 1.929824561403509e-07,
      "loss": 0.0627,
      "step": 1130
    },
    {
      "epoch": 9.958241758241758,
      "grad_norm": 0.22096213698387146,
      "learning_rate": 1.0526315789473685e-07,
      "loss": 0.0363,
      "step": 1135
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.03815875202417374,
      "learning_rate": 1.754385964912281e-08,
      "loss": 0.0069,
      "step": 1140
    }
  ],
  "logging_steps": 5,
  "max_steps": 1140,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3180407910236160.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
