{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 7.0,
  "eval_steps": 500,
  "global_step": 714,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.04938271604938271,
      "grad_norm": 5.236988544464111,
      "learning_rate": 1.9921568627450984e-05,
      "loss": 4.9423,
      "step": 5
    },
    {
      "epoch": 0.09876543209876543,
      "grad_norm": 8.832478523254395,
      "learning_rate": 1.9823529411764708e-05,
      "loss": 4.5072,
      "step": 10
    },
    {
      "epoch": 0.14814814814814814,
      "grad_norm": 7.03759241104126,
      "learning_rate": 1.9725490196078433e-05,
      "loss": 4.3347,
      "step": 15
    },
    {
      "epoch": 0.19753086419753085,
      "grad_norm": 4.77068567276001,
      "learning_rate": 1.9627450980392157e-05,
      "loss": 3.7038,
      "step": 20
    },
    {
      "epoch": 0.24691358024691357,
      "grad_norm": 6.592835903167725,
      "learning_rate": 1.9529411764705885e-05,
      "loss": 3.9988,
      "step": 25
    },
    {
      "epoch": 0.2962962962962963,
      "grad_norm": 5.965265274047852,
      "learning_rate": 1.943137254901961e-05,
      "loss": 3.8035,
      "step": 30
    },
    {
      "epoch": 0.345679012345679,
      "grad_norm": 5.966418266296387,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 3.3879,
      "step": 35
    },
    {
      "epoch": 0.3950617283950617,
      "grad_norm": 7.216070175170898,
      "learning_rate": 1.923529411764706e-05,
      "loss": 3.4011,
      "step": 40
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 5.362962245941162,
      "learning_rate": 1.9137254901960786e-05,
      "loss": 3.0191,
      "step": 45
    },
    {
      "epoch": 0.49382716049382713,
      "grad_norm": 5.190781593322754,
      "learning_rate": 1.903921568627451e-05,
      "loss": 2.8071,
      "step": 50
    },
    {
      "epoch": 0.5432098765432098,
      "grad_norm": 3.752577543258667,
      "learning_rate": 1.8941176470588238e-05,
      "loss": 2.5971,
      "step": 55
    },
    {
      "epoch": 0.5925925925925926,
      "grad_norm": 4.068707466125488,
      "learning_rate": 1.8843137254901962e-05,
      "loss": 2.4015,
      "step": 60
    },
    {
      "epoch": 0.6419753086419753,
      "grad_norm": 4.186023712158203,
      "learning_rate": 1.8745098039215686e-05,
      "loss": 2.3719,
      "step": 65
    },
    {
      "epoch": 0.691358024691358,
      "grad_norm": 4.100327491760254,
      "learning_rate": 1.8647058823529414e-05,
      "loss": 1.9586,
      "step": 70
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 4.498850345611572,
      "learning_rate": 1.854901960784314e-05,
      "loss": 2.0735,
      "step": 75
    },
    {
      "epoch": 0.7901234567901234,
      "grad_norm": 3.825822591781616,
      "learning_rate": 1.8450980392156866e-05,
      "loss": 1.6787,
      "step": 80
    },
    {
      "epoch": 0.8395061728395061,
      "grad_norm": 4.208443641662598,
      "learning_rate": 1.8352941176470587e-05,
      "loss": 1.7406,
      "step": 85
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 4.851659297943115,
      "learning_rate": 1.8254901960784315e-05,
      "loss": 1.5269,
      "step": 90
    },
    {
      "epoch": 0.9382716049382716,
      "grad_norm": 4.55633020401001,
      "learning_rate": 1.815686274509804e-05,
      "loss": 1.4541,
      "step": 95
    },
    {
      "epoch": 0.9876543209876543,
      "grad_norm": 3.898806571960449,
      "learning_rate": 1.8058823529411767e-05,
      "loss": 1.3633,
      "step": 100
    },
    {
      "epoch": 1.0296296296296297,
      "grad_norm": 3.9150285720825195,
      "learning_rate": 1.796078431372549e-05,
      "loss": 1.0122,
      "step": 105
    },
    {
      "epoch": 1.0790123456790124,
      "grad_norm": 3.8982677459716797,
      "learning_rate": 1.786274509803922e-05,
      "loss": 1.0192,
      "step": 110
    },
    {
      "epoch": 1.128395061728395,
      "grad_norm": 3.628774642944336,
      "learning_rate": 1.776470588235294e-05,
      "loss": 0.8776,
      "step": 115
    },
    {
      "epoch": 1.1777777777777778,
      "grad_norm": 4.061461925506592,
      "learning_rate": 1.7666666666666668e-05,
      "loss": 0.8145,
      "step": 120
    },
    {
      "epoch": 1.2271604938271605,
      "grad_norm": 6.409433364868164,
      "learning_rate": 1.7568627450980392e-05,
      "loss": 0.6701,
      "step": 125
    },
    {
      "epoch": 1.2765432098765432,
      "grad_norm": 3.9732887744903564,
      "learning_rate": 1.747058823529412e-05,
      "loss": 0.593,
      "step": 130
    },
    {
      "epoch": 1.325925925925926,
      "grad_norm": 3.281608819961548,
      "learning_rate": 1.7372549019607845e-05,
      "loss": 0.5117,
      "step": 135
    },
    {
      "epoch": 1.3753086419753087,
      "grad_norm": 3.134774684906006,
      "learning_rate": 1.7274509803921572e-05,
      "loss": 0.5496,
      "step": 140
    },
    {
      "epoch": 1.4246913580246914,
      "grad_norm": 5.925080299377441,
      "learning_rate": 1.7176470588235293e-05,
      "loss": 0.4126,
      "step": 145
    },
    {
      "epoch": 1.474074074074074,
      "grad_norm": 6.255024433135986,
      "learning_rate": 1.707843137254902e-05,
      "loss": 0.2791,
      "step": 150
    },
    {
      "epoch": 1.5234567901234568,
      "grad_norm": 2.608750104904175,
      "learning_rate": 1.6980392156862745e-05,
      "loss": 0.2786,
      "step": 155
    },
    {
      "epoch": 1.5728395061728395,
      "grad_norm": 4.029676914215088,
      "learning_rate": 1.6882352941176473e-05,
      "loss": 0.2144,
      "step": 160
    },
    {
      "epoch": 1.6222222222222222,
      "grad_norm": 3.8230080604553223,
      "learning_rate": 1.6784313725490198e-05,
      "loss": 0.1617,
      "step": 165
    },
    {
      "epoch": 1.671604938271605,
      "grad_norm": 2.736546277999878,
      "learning_rate": 1.6686274509803922e-05,
      "loss": 0.1516,
      "step": 170
    },
    {
      "epoch": 1.7209876543209877,
      "grad_norm": 2.966341257095337,
      "learning_rate": 1.658823529411765e-05,
      "loss": 0.1039,
      "step": 175
    },
    {
      "epoch": 1.7703703703703704,
      "grad_norm": 1.393344759941101,
      "learning_rate": 1.6490196078431374e-05,
      "loss": 0.0896,
      "step": 180
    },
    {
      "epoch": 1.819753086419753,
      "grad_norm": 3.355436325073242,
      "learning_rate": 1.63921568627451e-05,
      "loss": 0.0554,
      "step": 185
    },
    {
      "epoch": 1.8691358024691358,
      "grad_norm": 1.4614808559417725,
      "learning_rate": 1.6294117647058826e-05,
      "loss": 0.0423,
      "step": 190
    },
    {
      "epoch": 1.9185185185185185,
      "grad_norm": 4.440140724182129,
      "learning_rate": 1.619607843137255e-05,
      "loss": 0.0451,
      "step": 195
    },
    {
      "epoch": 1.9679012345679012,
      "grad_norm": 1.1581597328186035,
      "learning_rate": 1.6098039215686275e-05,
      "loss": 0.0565,
      "step": 200
    },
    {
      "epoch": 2.0098765432098764,
      "grad_norm": 0.5851479172706604,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.0238,
      "step": 205
    },
    {
      "epoch": 2.0592592592592593,
      "grad_norm": 1.5903055667877197,
      "learning_rate": 1.5901960784313727e-05,
      "loss": 0.0234,
      "step": 210
    },
    {
      "epoch": 2.108641975308642,
      "grad_norm": 1.8826005458831787,
      "learning_rate": 1.580392156862745e-05,
      "loss": 0.0174,
      "step": 215
    },
    {
      "epoch": 2.1580246913580248,
      "grad_norm": 0.9504191279411316,
      "learning_rate": 1.570588235294118e-05,
      "loss": 0.0139,
      "step": 220
    },
    {
      "epoch": 2.2074074074074073,
      "grad_norm": 0.308067262172699,
      "learning_rate": 1.5607843137254904e-05,
      "loss": 0.0099,
      "step": 225
    },
    {
      "epoch": 2.25679012345679,
      "grad_norm": 0.360137939453125,
      "learning_rate": 1.5509803921568628e-05,
      "loss": 0.0104,
      "step": 230
    },
    {
      "epoch": 2.3061728395061727,
      "grad_norm": 1.4959317445755005,
      "learning_rate": 1.5411764705882356e-05,
      "loss": 0.0173,
      "step": 235
    },
    {
      "epoch": 2.3555555555555556,
      "grad_norm": 1.2436625957489014,
      "learning_rate": 1.531372549019608e-05,
      "loss": 0.0091,
      "step": 240
    },
    {
      "epoch": 2.404938271604938,
      "grad_norm": 1.2710182666778564,
      "learning_rate": 1.5215686274509804e-05,
      "loss": 0.0072,
      "step": 245
    },
    {
      "epoch": 2.454320987654321,
      "grad_norm": 2.0713045597076416,
      "learning_rate": 1.511764705882353e-05,
      "loss": 0.0105,
      "step": 250
    },
    {
      "epoch": 2.5037037037037035,
      "grad_norm": 0.12925788760185242,
      "learning_rate": 1.5019607843137257e-05,
      "loss": 0.0198,
      "step": 255
    },
    {
      "epoch": 2.5530864197530865,
      "grad_norm": 2.387173652648926,
      "learning_rate": 1.4921568627450983e-05,
      "loss": 0.0092,
      "step": 260
    },
    {
      "epoch": 2.602469135802469,
      "grad_norm": 0.07514593750238419,
      "learning_rate": 1.4823529411764707e-05,
      "loss": 0.0094,
      "step": 265
    },
    {
      "epoch": 2.651851851851852,
      "grad_norm": 0.21462716162204742,
      "learning_rate": 1.4725490196078433e-05,
      "loss": 0.0055,
      "step": 270
    },
    {
      "epoch": 2.701234567901235,
      "grad_norm": 0.1345503330230713,
      "learning_rate": 1.4627450980392157e-05,
      "loss": 0.0041,
      "step": 275
    },
    {
      "epoch": 2.7506172839506173,
      "grad_norm": 0.1892070770263672,
      "learning_rate": 1.4529411764705883e-05,
      "loss": 0.007,
      "step": 280
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.22598831355571747,
      "learning_rate": 1.443137254901961e-05,
      "loss": 0.0039,
      "step": 285
    },
    {
      "epoch": 2.8493827160493828,
      "grad_norm": 0.3411387503147125,
      "learning_rate": 1.4333333333333334e-05,
      "loss": 0.0034,
      "step": 290
    },
    {
      "epoch": 2.8987654320987657,
      "grad_norm": 0.19289205968379974,
      "learning_rate": 1.423529411764706e-05,
      "loss": 0.0047,
      "step": 295
    },
    {
      "epoch": 2.948148148148148,
      "grad_norm": 0.5029206871986389,
      "learning_rate": 1.4137254901960786e-05,
      "loss": 0.0036,
      "step": 300
    },
    {
      "epoch": 2.9975308641975307,
      "grad_norm": 0.11229895800352097,
      "learning_rate": 1.403921568627451e-05,
      "loss": 0.0047,
      "step": 305
    },
    {
      "epoch": 3.039506172839506,
      "grad_norm": 0.09264405816793442,
      "learning_rate": 1.3941176470588236e-05,
      "loss": 0.0027,
      "step": 310
    },
    {
      "epoch": 3.088888888888889,
      "grad_norm": 0.26101550459861755,
      "learning_rate": 1.384313725490196e-05,
      "loss": 0.0024,
      "step": 315
    },
    {
      "epoch": 3.1382716049382715,
      "grad_norm": 0.07464733719825745,
      "learning_rate": 1.3745098039215687e-05,
      "loss": 0.0037,
      "step": 320
    },
    {
      "epoch": 3.1876543209876544,
      "grad_norm": 0.09267015010118484,
      "learning_rate": 1.3647058823529413e-05,
      "loss": 0.0028,
      "step": 325
    },
    {
      "epoch": 3.237037037037037,
      "grad_norm": 0.11854285001754761,
      "learning_rate": 1.3549019607843139e-05,
      "loss": 0.0021,
      "step": 330
    },
    {
      "epoch": 3.28641975308642,
      "grad_norm": 0.5280297994613647,
      "learning_rate": 1.3450980392156865e-05,
      "loss": 0.0056,
      "step": 335
    },
    {
      "epoch": 3.3358024691358024,
      "grad_norm": 0.05805031955242157,
      "learning_rate": 1.3352941176470588e-05,
      "loss": 0.0014,
      "step": 340
    },
    {
      "epoch": 3.3851851851851853,
      "grad_norm": 0.16119402647018433,
      "learning_rate": 1.3254901960784314e-05,
      "loss": 0.0018,
      "step": 345
    },
    {
      "epoch": 3.434567901234568,
      "grad_norm": 0.08144764602184296,
      "learning_rate": 1.315686274509804e-05,
      "loss": 0.0115,
      "step": 350
    },
    {
      "epoch": 3.4839506172839507,
      "grad_norm": 6.855391025543213,
      "learning_rate": 1.3058823529411766e-05,
      "loss": 0.0065,
      "step": 355
    },
    {
      "epoch": 3.533333333333333,
      "grad_norm": 0.0702267587184906,
      "learning_rate": 1.2960784313725492e-05,
      "loss": 0.0018,
      "step": 360
    },
    {
      "epoch": 3.582716049382716,
      "grad_norm": 0.09023638814687729,
      "learning_rate": 1.2862745098039218e-05,
      "loss": 0.0016,
      "step": 365
    },
    {
      "epoch": 3.6320987654320986,
      "grad_norm": 0.1062643751502037,
      "learning_rate": 1.276470588235294e-05,
      "loss": 0.0018,
      "step": 370
    },
    {
      "epoch": 3.6814814814814816,
      "grad_norm": 0.7256540656089783,
      "learning_rate": 1.2666666666666667e-05,
      "loss": 0.002,
      "step": 375
    },
    {
      "epoch": 3.730864197530864,
      "grad_norm": 0.042332299053668976,
      "learning_rate": 1.2568627450980393e-05,
      "loss": 0.0018,
      "step": 380
    },
    {
      "epoch": 3.780246913580247,
      "grad_norm": 0.07525854557752609,
      "learning_rate": 1.2470588235294119e-05,
      "loss": 0.0019,
      "step": 385
    },
    {
      "epoch": 3.8296296296296295,
      "grad_norm": 0.08314751833677292,
      "learning_rate": 1.2372549019607845e-05,
      "loss": 0.0023,
      "step": 390
    },
    {
      "epoch": 3.8790123456790124,
      "grad_norm": 0.10766331106424332,
      "learning_rate": 1.2274509803921571e-05,
      "loss": 0.0013,
      "step": 395
    },
    {
      "epoch": 3.928395061728395,
      "grad_norm": 0.6067099571228027,
      "learning_rate": 1.2176470588235294e-05,
      "loss": 0.0023,
      "step": 400
    },
    {
      "epoch": 3.977777777777778,
      "grad_norm": 0.04155047982931137,
      "learning_rate": 1.207843137254902e-05,
      "loss": 0.0016,
      "step": 405
    },
    {
      "epoch": 4.019753086419753,
      "grad_norm": 0.09038501232862473,
      "learning_rate": 1.1980392156862746e-05,
      "loss": 0.002,
      "step": 410
    },
    {
      "epoch": 4.069135802469136,
      "grad_norm": 0.044857483357191086,
      "learning_rate": 1.1882352941176472e-05,
      "loss": 0.0012,
      "step": 415
    },
    {
      "epoch": 4.118518518518519,
      "grad_norm": 0.06697401404380798,
      "learning_rate": 1.1784313725490198e-05,
      "loss": 0.0011,
      "step": 420
    },
    {
      "epoch": 4.167901234567902,
      "grad_norm": 0.06966014206409454,
      "learning_rate": 1.1686274509803922e-05,
      "loss": 0.0015,
      "step": 425
    },
    {
      "epoch": 4.217283950617284,
      "grad_norm": 0.06088710576295853,
      "learning_rate": 1.1588235294117648e-05,
      "loss": 0.0015,
      "step": 430
    },
    {
      "epoch": 4.266666666666667,
      "grad_norm": 0.10176379233598709,
      "learning_rate": 1.1490196078431373e-05,
      "loss": 0.0017,
      "step": 435
    },
    {
      "epoch": 4.3160493827160495,
      "grad_norm": 0.033135395497083664,
      "learning_rate": 1.1392156862745099e-05,
      "loss": 0.0013,
      "step": 440
    },
    {
      "epoch": 4.3654320987654325,
      "grad_norm": 0.05379144102334976,
      "learning_rate": 1.1294117647058825e-05,
      "loss": 0.0013,
      "step": 445
    },
    {
      "epoch": 4.4148148148148145,
      "grad_norm": 0.04023052006959915,
      "learning_rate": 1.119607843137255e-05,
      "loss": 0.0013,
      "step": 450
    },
    {
      "epoch": 4.4641975308641975,
      "grad_norm": 0.07039178162813187,
      "learning_rate": 1.1098039215686275e-05,
      "loss": 0.0014,
      "step": 455
    },
    {
      "epoch": 4.51358024691358,
      "grad_norm": 0.053613100200891495,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.0011,
      "step": 460
    },
    {
      "epoch": 4.562962962962963,
      "grad_norm": 0.06606247276067734,
      "learning_rate": 1.0901960784313726e-05,
      "loss": 0.0097,
      "step": 465
    },
    {
      "epoch": 4.612345679012345,
      "grad_norm": 0.055488135665655136,
      "learning_rate": 1.0803921568627452e-05,
      "loss": 0.0011,
      "step": 470
    },
    {
      "epoch": 4.661728395061728,
      "grad_norm": 0.050473250448703766,
      "learning_rate": 1.0705882352941178e-05,
      "loss": 0.0014,
      "step": 475
    },
    {
      "epoch": 4.711111111111111,
      "grad_norm": 0.038446709513664246,
      "learning_rate": 1.0607843137254902e-05,
      "loss": 0.001,
      "step": 480
    },
    {
      "epoch": 4.760493827160494,
      "grad_norm": 0.04159026965498924,
      "learning_rate": 1.0509803921568628e-05,
      "loss": 0.0012,
      "step": 485
    },
    {
      "epoch": 4.809876543209876,
      "grad_norm": 0.2528800070285797,
      "learning_rate": 1.0411764705882354e-05,
      "loss": 0.0016,
      "step": 490
    },
    {
      "epoch": 4.859259259259259,
      "grad_norm": 0.06843864172697067,
      "learning_rate": 1.031372549019608e-05,
      "loss": 0.0014,
      "step": 495
    },
    {
      "epoch": 4.908641975308642,
      "grad_norm": 0.05329040810465813,
      "learning_rate": 1.0215686274509805e-05,
      "loss": 0.0008,
      "step": 500
    },
    {
      "epoch": 4.958024691358025,
      "grad_norm": 0.10383725166320801,
      "learning_rate": 1.011764705882353e-05,
      "loss": 0.0012,
      "step": 505
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.06852629780769348,
      "learning_rate": 1.0019607843137255e-05,
      "loss": 0.0013,
      "step": 510
    },
    {
      "epoch": 5.049382716049383,
      "grad_norm": 0.04531140998005867,
      "learning_rate": 9.921568627450981e-06,
      "loss": 0.0011,
      "step": 515
    },
    {
      "epoch": 5.098765432098766,
      "grad_norm": 0.030796663835644722,
      "learning_rate": 9.823529411764706e-06,
      "loss": 0.0009,
      "step": 520
    },
    {
      "epoch": 5.148148148148148,
      "grad_norm": 0.02671876922249794,
      "learning_rate": 9.725490196078432e-06,
      "loss": 0.0009,
      "step": 525
    },
    {
      "epoch": 5.197530864197531,
      "grad_norm": 0.05649365857243538,
      "learning_rate": 9.627450980392158e-06,
      "loss": 0.0008,
      "step": 530
    },
    {
      "epoch": 5.246913580246914,
      "grad_norm": 0.026589645072817802,
      "learning_rate": 9.529411764705882e-06,
      "loss": 0.0007,
      "step": 535
    },
    {
      "epoch": 5.296296296296296,
      "grad_norm": 0.06974208354949951,
      "learning_rate": 9.431372549019608e-06,
      "loss": 0.001,
      "step": 540
    },
    {
      "epoch": 5.345679012345679,
      "grad_norm": 0.055157728493213654,
      "learning_rate": 9.333333333333334e-06,
      "loss": 0.0008,
      "step": 545
    },
    {
      "epoch": 5.395061728395062,
      "grad_norm": 0.057628024369478226,
      "learning_rate": 9.23529411764706e-06,
      "loss": 0.005,
      "step": 550
    },
    {
      "epoch": 5.444444444444445,
      "grad_norm": 0.1366814821958542,
      "learning_rate": 9.137254901960785e-06,
      "loss": 0.0011,
      "step": 555
    },
    {
      "epoch": 5.493827160493828,
      "grad_norm": 0.0556659996509552,
      "learning_rate": 9.03921568627451e-06,
      "loss": 0.0011,
      "step": 560
    },
    {
      "epoch": 5.54320987654321,
      "grad_norm": 0.024339230731129646,
      "learning_rate": 8.941176470588237e-06,
      "loss": 0.0009,
      "step": 565
    },
    {
      "epoch": 5.592592592592593,
      "grad_norm": 0.037529636174440384,
      "learning_rate": 8.843137254901961e-06,
      "loss": 0.0009,
      "step": 570
    },
    {
      "epoch": 5.6419753086419755,
      "grad_norm": 0.031658656895160675,
      "learning_rate": 8.745098039215687e-06,
      "loss": 0.001,
      "step": 575
    },
    {
      "epoch": 5.6913580246913575,
      "grad_norm": 0.03312436118721962,
      "learning_rate": 8.647058823529413e-06,
      "loss": 0.0013,
      "step": 580
    },
    {
      "epoch": 5.7407407407407405,
      "grad_norm": 0.02670367620885372,
      "learning_rate": 8.549019607843138e-06,
      "loss": 0.0007,
      "step": 585
    },
    {
      "epoch": 5.790123456790123,
      "grad_norm": 0.023464379832148552,
      "learning_rate": 8.450980392156864e-06,
      "loss": 0.0007,
      "step": 590
    },
    {
      "epoch": 5.839506172839506,
      "grad_norm": 0.06402900069952011,
      "learning_rate": 8.35294117647059e-06,
      "loss": 0.0011,
      "step": 595
    },
    {
      "epoch": 5.888888888888889,
      "grad_norm": 0.037681546062231064,
      "learning_rate": 8.254901960784314e-06,
      "loss": 0.0008,
      "step": 600
    },
    {
      "epoch": 5.938271604938271,
      "grad_norm": 0.03229004517197609,
      "learning_rate": 8.15686274509804e-06,
      "loss": 0.0009,
      "step": 605
    },
    {
      "epoch": 5.987654320987654,
      "grad_norm": 0.028445102274417877,
      "learning_rate": 8.058823529411766e-06,
      "loss": 0.0006,
      "step": 610
    },
    {
      "epoch": 6.029629629629629,
      "grad_norm": 0.056909624487161636,
      "learning_rate": 7.96078431372549e-06,
      "loss": 0.0005,
      "step": 615
    },
    {
      "epoch": 6.079012345679012,
      "grad_norm": 0.12230372428894043,
      "learning_rate": 7.862745098039217e-06,
      "loss": 0.0008,
      "step": 620
    },
    {
      "epoch": 6.128395061728395,
      "grad_norm": 0.03077307716012001,
      "learning_rate": 7.764705882352941e-06,
      "loss": 0.0006,
      "step": 625
    },
    {
      "epoch": 6.177777777777778,
      "grad_norm": 0.0642404556274414,
      "learning_rate": 7.666666666666667e-06,
      "loss": 0.0007,
      "step": 630
    },
    {
      "epoch": 6.22716049382716,
      "grad_norm": 0.06103676185011864,
      "learning_rate": 7.5686274509803925e-06,
      "loss": 0.0007,
      "step": 635
    },
    {
      "epoch": 6.276543209876543,
      "grad_norm": 0.03413417562842369,
      "learning_rate": 7.4705882352941185e-06,
      "loss": 0.0006,
      "step": 640
    },
    {
      "epoch": 6.325925925925926,
      "grad_norm": 0.04410065710544586,
      "learning_rate": 7.372549019607845e-06,
      "loss": 0.0007,
      "step": 645
    },
    {
      "epoch": 6.375308641975309,
      "grad_norm": 0.01746187172830105,
      "learning_rate": 7.274509803921569e-06,
      "loss": 0.0007,
      "step": 650
    },
    {
      "epoch": 6.424691358024692,
      "grad_norm": 0.027480339631438255,
      "learning_rate": 7.176470588235295e-06,
      "loss": 0.0006,
      "step": 655
    },
    {
      "epoch": 6.474074074074074,
      "grad_norm": 0.07254683971405029,
      "learning_rate": 7.07843137254902e-06,
      "loss": 0.0008,
      "step": 660
    },
    {
      "epoch": 6.523456790123457,
      "grad_norm": 0.02719944156706333,
      "learning_rate": 6.9803921568627454e-06,
      "loss": 0.0006,
      "step": 665
    },
    {
      "epoch": 6.57283950617284,
      "grad_norm": 0.019277645274996758,
      "learning_rate": 6.8823529411764715e-06,
      "loss": 0.0007,
      "step": 670
    },
    {
      "epoch": 6.622222222222222,
      "grad_norm": 0.02760571427643299,
      "learning_rate": 6.784313725490197e-06,
      "loss": 0.0009,
      "step": 675
    },
    {
      "epoch": 6.671604938271605,
      "grad_norm": 0.04139028117060661,
      "learning_rate": 6.686274509803922e-06,
      "loss": 0.0009,
      "step": 680
    },
    {
      "epoch": 6.720987654320988,
      "grad_norm": 0.02818700484931469,
      "learning_rate": 6.588235294117647e-06,
      "loss": 0.0006,
      "step": 685
    },
    {
      "epoch": 6.770370370370371,
      "grad_norm": 0.019241824746131897,
      "learning_rate": 6.490196078431373e-06,
      "loss": 0.0008,
      "step": 690
    },
    {
      "epoch": 6.8197530864197535,
      "grad_norm": 0.03445253148674965,
      "learning_rate": 6.3921568627450984e-06,
      "loss": 0.0007,
      "step": 695
    },
    {
      "epoch": 6.869135802469136,
      "grad_norm": 0.04830297827720642,
      "learning_rate": 6.294117647058824e-06,
      "loss": 0.0006,
      "step": 700
    },
    {
      "epoch": 6.9185185185185185,
      "grad_norm": 0.02782084420323372,
      "learning_rate": 6.19607843137255e-06,
      "loss": 0.0033,
      "step": 705
    },
    {
      "epoch": 6.9679012345679014,
      "grad_norm": 0.02407059073448181,
      "learning_rate": 6.098039215686276e-06,
      "loss": 0.0007,
      "step": 710
    }
  ],
  "logging_steps": 5,
  "max_steps": 1020,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2048823159914496.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
